{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import cfg_fix\n",
    "from cfg_fix import parse_grammar, CFG\n",
    "from pprint import pprint\n",
    "# The printing and tracing functionality is in a separate file in order\n",
    "#  to make this file easier to read\n",
    "from cky_print import CKY_pprint, CKY_log, Cell__str__, Cell_str, Cell_log\n",
    "\n",
    "class CKY:\n",
    "    \"\"\"An implementation of the Cocke-Kasami-Younger (bottom-up) CFG recogniser.\n",
    "\n",
    "    Goes beyond strict CKY's insistance on Chomsky Normal Form.\n",
    "    It allows arbitrary unary productions, not just NT->T\n",
    "    ones, that is X -> Y with either Y -> A B or Y -> Z .\n",
    "    It also allows mixed binary productions, that is NT -> NT T or -> T NT\"\"\"\n",
    "\n",
    "    def __init__(self,grammar):\n",
    "        '''Create an extended CKY processor for a particular grammar\n",
    "\n",
    "        Grammar is an NLTK CFG\n",
    "        consisting of unary and binary rules (no empty rules,\n",
    "        no more than two symbols on the right-hand side\n",
    "\n",
    "        (We use \"symbol\" throughout this code to refer to _either_ a string or\n",
    "        an nltk.grammar.Nonterminal, that is, the two thinegs we find in\n",
    "        nltk.grammar.Production)\n",
    "\n",
    "        :type grammar: nltk.grammar.CFG, as fixed by cfg_fix\n",
    "        :param grammar: A context-free grammar\n",
    "        :return: none'''\n",
    "\n",
    "        self.verbose=False\n",
    "        assert(isinstance(grammar,CFG))\n",
    "        self.grammar=grammar\n",
    "        # split and index the grammar\n",
    "        self.buildIndices(grammar.productions())\n",
    "\n",
    "    def buildIndices(self,productions):\n",
    "        '''\n",
    "        args: list of productions with terminal and non-terminal components on lhs and rhs of every production.\n",
    "            production - Each production maps a single symbol on the \"left-hand side\"(non-terminal) to a sequence of symbols on the \"right-hand side\"(terminals or non-terminal)\n",
    "        \n",
    "        Since it is a bottom-up approach, the rhs of the productions are made keys of the dictionary and lhs are the values. \n",
    "        This function segregates productions into unary and binary rules by checking the rhs length and returning them.\n",
    "\n",
    "        returns: Two dictionaries those have unary and binary grammar rules.\n",
    "        '''\n",
    "        self.unary=defaultdict(list)\n",
    "        self.binary=defaultdict(list)\n",
    "        for production in productions:\n",
    "            rhs=production.rhs()\n",
    "            lhs=production.lhs()\n",
    "            assert(len(rhs)>0 and len(rhs)<=2) # Cross-checking to CNF rule that states only 1 or 2 child(ren) are allowed \n",
    "            if len(rhs)==1:\n",
    "                self.unary[rhs[0]].append(lhs)\n",
    "            else:\n",
    "                self.binary[rhs].append(lhs)\n",
    "\n",
    "    def recognise(self,tokens,verbose=False):\n",
    "        '''replace/expand this docstring. Your docs need NOT\n",
    "        say anything more about the verbose option.\n",
    "\n",
    "        Initialise a n * n+1 matrix to create a upper traingular matrix (or a parse traingle/chart) from the sentence,\n",
    "        then run the CKY algorithm over it\n",
    "\n",
    "        :type tokens:\n",
    "        :param tokens:\n",
    "        :type verbose: bool\n",
    "        :param verbose: show debugging output if True, defaults to False\n",
    "        :rtype: \n",
    "        :return:\n",
    "\n",
    "        '''\n",
    "        self.verbose=verbose\n",
    "        self.words = tokens\n",
    "        self.n = len(self.words)+1\n",
    "        self.matrix = []\n",
    "        # We index by row, then column\n",
    "        #  So Y below is 1,2 and Z is 0,3\n",
    "        #    1   2   3  ...\n",
    "        # 0  .   .   Z\n",
    "        # 1      Y   .\n",
    "        # 2          .\n",
    "        # ...\n",
    "        for r in range(self.n-1):\n",
    "             # rows\n",
    "             row=[]\n",
    "             for c in range(self.n):\n",
    "                 # columns\n",
    "                 if c>r:\n",
    "                     # This is one we care about, add a cell\n",
    "                     row.append(Cell(r,c,self))\n",
    "                 else:\n",
    "                     # just a filler\n",
    "                     row.append(None)\n",
    "             self.matrix.append(row)\n",
    "        self.unaryFill()\n",
    "        self.binaryScan()\n",
    "        # Replace the line below for Q6\n",
    "        return self.grammar.start() in self.matrix[0][self.n-1].labels()\n",
    "\n",
    "    def unaryFill(self):\n",
    "        '''\n",
    "        args: none\n",
    "\n",
    "        This method fills all the words in the bottom most cells of the parse tree \n",
    "        i.e., words are added on the diagonal of the upper triangular matrix.\n",
    "\n",
    "        returns: none\n",
    "        '''\n",
    "        for r in range(self.n-1):\n",
    "            cell=self.matrix[r][r+1]\n",
    "            word=self.words[r]\n",
    "            cell.addLabel(word)\n",
    "            cell.unaryUpdate(word)\n",
    "\n",
    "    def binaryScan(self):\n",
    "        '''(The heart of the implementation.)\n",
    "\n",
    "Postcondition: the matrix has been filled with all constituents that\n",
    "can be built from the input words and grammar.\n",
    "\n",
    "How: Starting with constituents of length 2 (because length 1 has been\n",
    "done already), proceed across the upper-right diagonals from left to\n",
    "right and in increasing order of constituent length. Call maybeBuild\n",
    "for each possible choice of (start, mid, end) positions to try to\n",
    "build something at those positions.\n",
    "\n",
    "        '''\n",
    "        for span in range(2, self.n):\n",
    "            for start in range(self.n-span):\n",
    "                end = start + span\n",
    "                for mid in range(start+1, end):\n",
    "                    self.maybeBuild(start, mid, end)\n",
    "\n",
    "    def maybeBuild(self, start, mid, end):\n",
    "        '''\n",
    "        args: \n",
    "        start - m\n",
    "        mid - m + 1\n",
    "        end - m + 2\n",
    "        \n",
    "        Checks for every co-occurring adjacent terminals or non-terminals (rhs elements) in the binary dictionary and \n",
    "        returns the value of the rhs key from the binary dictionary. \n",
    "        Then it is passed back to the unaryUpdate function where the found elements are stacked above the current co-occurring adjacent terminals or non-terminals in the matrix.\n",
    "\n",
    "        '''\n",
    "        self.log(\"%s--%s--%s:\",start, mid, end)\n",
    "        cell=self.matrix[start][end]\n",
    "        for s1 in self.matrix[start][mid].labels():\n",
    "            for s2 in self.matrix[mid][end].labels():\n",
    "                if (s1,s2) in self.binary:\n",
    "                    for s in self.binary[(s1,s2)]:\n",
    "                        self.log(\"%s -> %s %s\", s, s1, s2, indent=1)\n",
    "                        cell.addLabel(s)\n",
    "                        cell.unaryUpdate(s,1)\n",
    "\n",
    "# helper methods from cky_print\n",
    "CKY.pprint=CKY_pprint\n",
    "CKY.log=CKY_log\n",
    "\n",
    "class Cell:\n",
    "    '''A cell in a CKY matrix'''\n",
    "    def __init__(self,row,column,matrix):\n",
    "        self._row=row\n",
    "        self._column=column\n",
    "        self.matrix=matrix\n",
    "        self._labels=[]\n",
    "\n",
    "    def addLabel(self,label):\n",
    "        self._labels.append(label)\n",
    "\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    def unaryUpdate(self,symbol,depth=0,recursive=False):\n",
    "        '''\n",
    "        args: terminal (word from the sentence, if depth is 0) / non-terminals if depth is > 0\n",
    "\n",
    "        Logs the terminal/non-terminal in the Cell. \n",
    "        Then checks for the terminal or non-terminal in the unary dictionary, \n",
    "        where terminal or non-terminal (patterns of interest) will be stored as the values with keys as the parent of the rule \n",
    "        (lhs in a production )\n",
    "        '''\n",
    "        if not recursive:\n",
    "            self.log(str(symbol),indent=depth)\n",
    "        if symbol in self.matrix.unary:\n",
    "            for parent in self.matrix.unary[symbol]:\n",
    "                self.matrix.log(\"%s -> %s\",parent,symbol,indent=depth+1)\n",
    "                self.addLabel(parent)\n",
    "                self.unaryUpdate(parent,depth+1,True)\n",
    "\n",
    "# helper methods from cky_print\n",
    "Cell.__str__=Cell__str__\n",
    "Cell.str=Cell_str\n",
    "Cell.log=Cell_log\n",
    "\n",
    "class Label:\n",
    "    '''A label for a substring in a CKY chart Cell\n",
    "\n",
    "    Includes a terminal or non-terminal symbol, possibly other\n",
    "    information.  Add more to this docstring when you start using this\n",
    "    class'''\n",
    "    def __init__(self,symbol,\n",
    "                 # Fill in here, if more needed\n",
    "                 ):\n",
    "        '''Create a label from a symbol and ...\n",
    "        :type symbol: a string (for terminals) or an nltk.grammar.Nonterminal\n",
    "        :param symbol: a terminal or non-terminal\n",
    "        '''\n",
    "        self._symbol=symbol\n",
    "        # augment as appropriate, with comments\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self._symbol)\n",
    "\n",
    "    def __eq__(self,other):\n",
    "        '''How to test for equality -- other must be a label,\n",
    "        and symbols have to be equal'''\n",
    "        assert isinstance(other,Label)\n",
    "        return self._symbol==other._symbol\n",
    "\n",
    "    def symbol(self):\n",
    "        return self._symbol\n",
    "    # Add more methods as required, with docstring and comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix buggy NLTK 3 :-(\n",
    "# different fixes for different versions :-((((\n",
    "import re,sys\n",
    "import nltk\n",
    "from nltk.grammar import _ARROW_RE, _PROBABILITY_RE, _DISJUNCTION_RE, Production\n",
    "from nltk.draw import CFGEditor\n",
    "from nltk import Tree\n",
    "ARROW = u'\\u2192'\n",
    "TOKEN = u'([\\\\w ]|\\\\\\\\((x[0-9a-f][0-9a-f])|(u[0-9a-f][0-9a-f][0-9a-f][0-9a-f])))+'\n",
    "CFGEditor.ARROW = ARROW\n",
    "CFGEditor._TOKEN_RE=re.compile(u\"->|u?'\"+TOKEN+u\"'|u?\\\"\"+TOKEN+u\"\\\"|\\\\w+|(\"+ARROW+u\")\")\n",
    "CFGEditor._PRODUCTION_RE=re.compile(u\"(^\\s*\\w+\\s*)\" +\n",
    "                  u\"(->|(\"+ARROW+\"))\\s*\" +\n",
    "                  u\"((u?'\"+TOKEN+\"'|u?\\\"\"+TOKEN+\"\\\"|''|\\\"\\\"|\\w+|\\|)\\s*)*$\")\n",
    "nltk.grammar._TERMINAL_RE = re.compile(u'( u?\"[^\"]+\" | u?\\'[^\\']+\\' ) \\s*', re.VERBOSE)\n",
    "nltk.grammar._ARROR_RE = re.compile(u'\\s* (->|'+ARROW+') \\s*', re.VERBOSE)\n",
    "\n",
    "from nltk.grammar import _TERMINAL_RE\n",
    "\n",
    "if sys.version_info[0]>2 or sys.version_info[1]>6:\n",
    "    from nltk.grammar import CFG, ProbabilisticProduction as FixPP\n",
    "    parse_grammar=CFG.fromstring\n",
    "    Tree.parse=Tree.fromstring\n",
    "else:\n",
    "    from nltk.grammar import WeightedProduction as FixPP, ContextFreeGrammar as CFG\n",
    "    from nltk import parse_cfg\n",
    "    parse_grammar=parse_cfg\n",
    "\n",
    "def fix_parse_production(line, nonterm_parser, probabilistic=False):\n",
    "    \"\"\"\n",
    "    Parse a grammar rule, given as a string, and return\n",
    "    a list of productions.\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "\n",
    "    # Parse the left-hand side.\n",
    "    lhs, pos = nonterm_parser(line, pos)\n",
    "\n",
    "    # Skip over the arrow.\n",
    "    m = _ARROW_RE.match(line, pos)\n",
    "    if not m: raise ValueError('Expected an arrow')\n",
    "    pos = m.end()\n",
    "\n",
    "    # Parse the right hand side.\n",
    "    probabilities = [0.0]\n",
    "    rhsides = [[]]\n",
    "    while pos < len(line):\n",
    "        # Probability.\n",
    "        m = _PROBABILITY_RE.match(line, pos)\n",
    "        if probabilistic and m:\n",
    "            pos = m.end()\n",
    "            probabilities[-1] = float(m.group(1)[1:-1])\n",
    "            if probabilities[-1] > 1.0:\n",
    "                raise ValueError('Production probability %f, '\n",
    "                                 'should not be greater than 1.0' %\n",
    "                                 (probabilities[-1],))\n",
    "\n",
    "        # String -- add terminal.\n",
    "        elif (line[pos] in \"\\'\\\"\" or line[pos:pos+2] in ('u\"',\"u'\")):\n",
    "            m = _TERMINAL_RE.match(line, pos)\n",
    "            if not m: raise ValueError('Unterminated string')\n",
    "            rhsides[-1].append(eval(m.group(1)))\n",
    "            pos = m.end()\n",
    "\n",
    "        # Vertical bar -- start new rhside.\n",
    "        elif line[pos] == '|':\n",
    "            m = _DISJUNCTION_RE.match(line, pos)\n",
    "            probabilities.append(0.0)\n",
    "            rhsides.append([])\n",
    "            pos = m.end()\n",
    "\n",
    "        # Anything else -- nonterminal.\n",
    "        else:\n",
    "            nonterm, pos = nonterm_parser(line, pos)\n",
    "            rhsides[-1].append(nonterm)\n",
    "\n",
    "    if probabilistic:\n",
    "        return [FixPP(lhs, rhs, prob=probability)\n",
    "                for (rhs, probability) in zip(rhsides, probabilities)]\n",
    "    else:\n",
    "        return [Production(lhs, rhs) for rhs in rhsides]\n",
    "\n",
    "if sys.version_info[0]>2 or sys.version_info[1]>6:\n",
    "    nltk.grammar._read_production=fix_parse_production\n",
    "else:\n",
    "    nltk.grammar.parse_production=fix_parse_production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 27 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    NP -> Det Nom\n",
      "    NP -> Nom\n",
      "    NP -> NP PP\n",
      "    Det -> NP \"'s\"\n",
      "    Nom -> N SRel\n",
      "    Nom -> N\n",
      "    VP -> Vi\n",
      "    VP -> Vt NP\n",
      "    VP -> VP PP\n",
      "    PP -> Prep NP\n",
      "    SRel -> Relpro VP\n",
      "    Det -> 'a'\n",
      "    Det -> 'the'\n",
      "    N -> 'fish'\n",
      "    N -> 'frogs'\n",
      "    N -> 'soup'\n",
      "    N -> 'children'\n",
      "    N -> 'books'\n",
      "    Prep -> 'in'\n",
      "    Prep -> 'for'\n",
      "    Vt -> 'saw'\n",
      "    Vt -> 'ate'\n",
      "    Vt -> 'read'\n",
      "    Vi -> 'fish'\n",
      "    Vi -> 'swim'\n",
      "    Relpro -> 'that'\n",
      "0,1: the\n",
      " Det -> the\n",
      "1,2: frogs\n",
      " N -> frogs\n",
      "  Nom -> N\n",
      "   NP -> Nom\n",
      "2,3: swim\n",
      " Vi -> swim\n",
      "  VP -> Vi\n",
      "0--1--2:\n",
      " NP -> Det Nom\n",
      " 0,2: NP\n",
      "1--2--3:\n",
      " S -> NP VP\n",
      " 1,3: S\n",
      "0--1--3:\n",
      "0--2--3:\n",
      " S -> NP VP\n",
      " 0,3: S\n",
      "     1       2       3   \n",
      "\n",
      "the Det|     NP|      S\n",
      "  -------+-------+-------\n",
      "\n",
      "       | Nom NP|       \n",
      "1\n",
      "       |frogs N|      S\n",
      "  -------+-------+-------\n",
      "\n",
      "       |       |     VP\n",
      "2\n",
      "       |       |swim Vi\n"
     ]
    }
   ],
   "source": [
    "''' Starting point for ANLP 2017 assignment 2: CKY parsing'''\n",
    "import re\n",
    "import cfg_fix\n",
    "from cfg_fix import parse_grammar, Tree\n",
    "from cky import CKY\n",
    "\n",
    "def tokenise(tokenstring):\n",
    "  '''Split a string into a list of tokens\n",
    "\n",
    "  We treat punctuation as\n",
    "  separate tokens, and split contractions into their parts.\n",
    "  \n",
    "  So for example \"I'm leaving.\" --> [\"I\",\"'m\",\"leaving\",\".\"]\n",
    "  \n",
    "  :type tokenstring: str\n",
    "  :param tokenstring: the string to be tokenised\n",
    "  :rtype: list(str)\n",
    "  :return: the tokens found in tokenstring'''\n",
    "  return re.findall(\n",
    "        # We use three sub-patterns:\n",
    "        #   one for words and the first half of possessives\n",
    "        #   one for the rest of possessives\n",
    "        #   one for punctuation\n",
    "        r\"[-\\w]+|'\\w+|[^-\\w\\s]+\",\n",
    "        tokenstring,\n",
    "        re.U # Use unicode classes, otherwise we would split\n",
    "             # \"são jaques\" into [\"s\", \"ão\",\"jaques\"]\n",
    "        )\n",
    "\n",
    "grammar=parse_grammar(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det Nom | Nom | NP PP\n",
    "Det -> NP \"'s\"\n",
    "Nom -> N SRel | N\n",
    "VP -> Vi | Vt NP | VP PP\n",
    "PP -> Prep NP\n",
    "SRel -> Relpro VP\n",
    "Det -> 'a' | 'the'\n",
    "N -> 'fish' | 'frogs' | 'soup' | 'children' | 'books'\n",
    "Prep -> 'in' | 'for'\n",
    "Vt -> 'saw' | 'ate' | 'read'\n",
    "Vi -> 'fish' | 'swim'\n",
    "Relpro -> 'that'\n",
    "\"\"\")\n",
    "\n",
    "# Use this grammar for the rest of the assignment\n",
    "grammar2=parse_grammar([\n",
    "\"S -> Sdecl '.' | Simp '.' | Sq '?' \",\n",
    "\"Sdecl -> NP VP\",\n",
    "\"Simp -> VP\",\n",
    "\"Sq -> Sqyn | Swhadv\",\n",
    "\"Sqyn -> Mod Sdecl | Aux Sdecl\",\n",
    "\"Swhadv -> WhAdv Sqyn\",\n",
    "\"Sc -> Subconj Sdecl\",\n",
    "\"NP -> PropN | Pro | NP0 \", # NP that allows no modification\n",
    "\"NP0 -> NP1 | NP0 PP\",\n",
    "\"NP1 -> Det N2sc | N2mp | Sc\",\n",
    "\"N2sc -> Adj N2sc | Nsc | N3 Nsc\",\n",
    "\"N2mp -> Adj N2mp | Nmp | N3 Nmp\",\n",
    "\"N3 -> N | N3 N\",\n",
    "\"N -> Nsc | Nmp\",\n",
    "\"VP -> VPi | VPt | VPdt | Mod VP | VP Adv | VP PP\",\n",
    "\"VPi -> Vi\", # intransitive\n",
    "\"VPt -> Vt NP\", # transitive\n",
    "\"VPdt -> VPo PP\", # ditransitive, obligatory NP (obj.) & PP complements\n",
    "\"VPdt -> VPio NP\", # ditransitive, obligatory NP (iobj.) & NP (obj)\n",
    "\"VPo -> Vdt NP\", # direct object of ditransitive\n",
    "\"VPio -> Vdt NP\", # indirect obj. part of dative-shifted ditransitive\n",
    "\"PP -> Prep NP\",\n",
    "\"Det -> 'a' | 'the'\",\n",
    "\"Nmp -> 'salad' | 'mushrooms'\",  #mass or plural nouns\n",
    "\"Nsc -> 'book' | 'fork' | 'flight' | 'salad' | 'drawing'\",  #singular count nouns\n",
    "\"Prep -> 'to' | 'with'\",\n",
    "\"Vi -> 'ate'\", #intransitive\n",
    "\"Vt -> 'ate' | 'book' | 'Book' | 'gave' | 'told'\", #transitive\n",
    "\"Vdt -> 'gave' | 'told' \", #ditransitive\n",
    "\"Subconj -> 'that'\", #subordinating conjunction\n",
    "\"Mod -> 'Can' | 'will'\", #modal verbs\n",
    "\"Aux -> 'did' \", #auxiliary verbs\n",
    "\"WhAdv -> 'Why'\",\n",
    "\"PropN -> 'John' | 'Mary' | 'NYC' | 'London'\",\n",
    "\"Adj -> 'nice' | 'drawing'\",\n",
    "\"Pro -> 'you' | 'he'\",\n",
    "\"Adv -> 'today'\"\n",
    "])\n",
    "\n",
    "print(grammar) #the simpler grammar\n",
    "chart=CKY(grammar)\n",
    " #this illustrates tracing of a very simple sentence; feel free to try others.\n",
    "chart.recognise(tokenise(\"the frogs swim\"),True)\n",
    "chart.pprint()\n",
    "\n",
    "#build a chart with the larger grammar\n",
    "chart2=CKY(grammar2)\n",
    "\n",
    "# Note, please do _not_ use the Tree.draw() method uncommented\n",
    "# _anywhere in this file_ (you are encouraged to use it in preparing\n",
    "# your report).\n",
    "\n",
    "# The sentences to examine.\n",
    "#\n",
    "# for s in [\"John gave a book to Mary.\",\n",
    "#           \"John gave Mary a book.\",\n",
    "#           \"John gave Mary a nice drawing book.\",\n",
    "#           \"John ate salad with mushrooms with a fork.\",\n",
    "#           \"Book a flight to NYC.\",\n",
    "#           \"Can you book a flight to London?\",\n",
    "#           \"Why did John book the flight?\",\n",
    "#           \"John told Mary that he will book a flight today.\"]:\n",
    "#     print(s, chart2.recognise(tokenise(s)))\n",
    "\n",
    "# Task 5\n",
    "# for s in [...]:\n",
    "#     print(s, chart2.parse(tokenise(s)))\n",
    "#     print(chart2.firstTree().pprint())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
